{"componentChunkName":"component---src-templates-post-jsx","path":"/pages/epoch-batchsize-iteration","result":{"data":{"site":{"siteMetadata":{"title":"ij.Log"}},"markdownRemark":{"html":"<h1 id=\"데이터-세분화를-통한-학습-효율\" style=\"position:relative;\"><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%84%B8%EB%B6%84%ED%99%94%EB%A5%BC-%ED%86%B5%ED%95%9C-%ED%95%99%EC%8A%B5-%ED%9A%A8%EC%9C%A8\" aria-label=\"데이터 세분화를 통한 학습 효율 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>데이터 세분화를 통한 학습 효율</h1>\n<ul>\n<li>학습 과정 중 <strong>최적화 (optimization)</strong> 는 여러 번의 학습 과정을 거치며,</li>\n<li>한 번의 학습 과정은 학습 데이터를 어떻게 나누냐에 따라 세분화 된다.</li>\n</ul>\n<hr>\n<br/>\n<h1 id=\"1-epoch\" style=\"position:relative;\"><a href=\"#1-epoch\" aria-label=\"1 epoch permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. epoch</h1>\n<ul>\n<li>\n<p>전체 데이터 셋에 대하여 한 번 학습을 완료</p>\n<ul>\n<li>인공신경망 기본 알고리즘: 역전파 알고리즘 (forward pass + backward pass)</li>\n<li>epoch 는 전체 데이터 셋에 대하여 forward pass 와 backward pass 가 완료된 상태</li>\n</ul>\n</li>\n<li>epoch = 30 으로 설정하여 학습 → 전체 데이터를 30 번 사용하여 학습</li>\n<li>epoch 가 너무 큼 → overfitting</li>\n<li>epoch 가 너무 작음 → underfitting</li>\n</ul>\n<h1 id=\"2-iteration--batch-size\" style=\"position:relative;\"><a href=\"#2-iteration--batch-size\" aria-label=\"2 iteration  batch size permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. iteration / batch size</h1>\n<h4 id=\"iteration\" style=\"position:relative;\"><a href=\"#iteration\" aria-label=\"iteration permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>iteration</h4>\n<ul>\n<li>한 번의 학습 과정 (epoch) 에서 데이터를 몇번 나눠서 줄 것인가를 설정하는 변수</li>\n<li>epoch 를 몇번에 나누어서 실행할 것인지 결정</li>\n<li>\n<p><strong>학습 영향</strong></p>\n<ul>\n<li>전체 데이터에 대한 오차 (cost) 총합을 가지고 backpropagation을 수행하면 weight가 한번에 크게 변할 수 있음</li>\n<li>gradient의 이동 폭이 커서 global minimum을 지나칠 수도 있기에 gradient를 반복적으로 조금씩 이동하게 하는 역할을 수행</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"batch-size\" style=\"position:relative;\"><a href=\"#batch-size\" aria-label=\"batch size permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>batch size</h4>\n<ul>\n<li>각 iteration 마다 주는 데이터의 사이즈를 결정</li>\n<li>\n<p><strong>학습 영향</strong></p>\n<ul>\n<li>batch를 너무 작게하면 iteration이 증가하여 학습 시간(forward + backward propagation)이 증가할 수 있음</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 690px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/85972dda534dda203e36afe50d089060/00b70/1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.41618497109826%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQoz62RbU+DMBSF+f9/y8w43bLMF4bBKRntKG2hL1BwH+BIO7IvRt0Hm9w8uT3n3pO0kRAchBDwkuE/TqS0xcN6i31G0LgTlOlgmh7KdtATtZ2r+Z3e77oTIspNWPBOKtj288IPWkPULQjTOE4eJi0KYcGrJvReo6WBVGeW833kDabtkRcqLDrMzJmaUnsI1UHqDrXpw5BfUEpPBzYFVPpMH+D1qHUdNo8xdq97vOxSxMkbnuM09PerDW4WSyzuVrhdroPmPZdK0svM9inBIaeIxnH88YGt0WDFEZyX8J931adYY5BlGSilEFLCOReEYRi+mX34X/UFXWIV3+sys7sAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image\"\n        title=\"image\"\n        src=\"/static/85972dda534dda203e36afe50d089060/1e043/1.png\"\n        srcset=\"/static/85972dda534dda203e36afe50d089060/991de/1.png 173w,\n/static/85972dda534dda203e36afe50d089060/e4d6b/1.png 345w,\n/static/85972dda534dda203e36afe50d089060/1e043/1.png 690w,\n/static/85972dda534dda203e36afe50d089060/00b70/1.png 733w\"\n        sizes=\"(max-width: 690px) 100vw, 690px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h1 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>summary</h1>\n<ul>\n<li>Total Dataset = Batch Size * mini batch</li>\n<li>1 Epoch = Batch Size * Iteration</li>\n</ul>","excerpt":"데이터 세분화를 통한 학습 효율 학습 과정 중 최적화 (optimization) 는 여러 번의 학습 과정을 거치며, 한 번의 학습 과정은 학습 데이터를 어떻게 나누냐에 따라 세분화 된다. 1. epoch…","frontmatter":{"date":"2020-11-09","title":"[AI, ML] epoch, batch size, iteration","tags":["AI","Machine Learning"],"keywords":null},"fields":{"slug":"/pages/epoch-batchsize-iteration"}}},"pageContext":{"slug":"/pages/epoch-batchsize-iteration"}},"staticQueryHashes":["2827402515","694178885"]}