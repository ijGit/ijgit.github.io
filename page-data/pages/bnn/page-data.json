{"componentChunkName":"component---src-templates-post-jsx","path":"/pages/bnn","result":{"data":{"site":{"siteMetadata":{"title":"ij.Log"}},"markdownRemark":{"html":"<h2 id=\"abstract\" style=\"position:relative;\"><a href=\"#abstract\" aria-label=\"abstract permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Abstract</h2>\n<p><strong>BNNs (Binarized Neural Networks)</strong></p>\n<ul>\n<li>neural networks with <strong>binary wrights and activations</strong> at run-time</li>\n<li>at trining-time the binary weights and activations are userd for computing the gradients</li>\n</ul>\n<p><strong>방법</strong></p>\n<ul>\n<li>forward pass 동안, 메모리 사이즈와 접근을 줄이고, 대부분의 산술 연산을 비트 연산으로 대체함으로써 전력 효율성을 개선할 수 있다.</li>\n</ul>\n<p><strong>검증</strong></p>\n<ul>\n<li>two sets of experiments on Torch7 and Theano frameworks</li>\n<li>검증 결과 MNIST, CIFAR-10, SVHN datasets 에서 매우 좋은 결과를 얻었다.</li>\n<li>\n<p><code class=\"language-text\">binary matrix multiplication GPU kernel</code> 사용</p>\n<ul>\n<li>분류 정확도의 손실 없이, 최적화 되지 않은 GPU 커널보다 MNIST BNN 을 7배 빠르게 돌릴 수 있다.</li>\n<li>해당 훈련과 구동을 위한 BNNs 코드는 on-line 으로도 실행 가능하다.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>Deep Neural Networks (DNNs)은 넓은 범위의 task 에서 실질적으로 인공지능 (Artificial Intelligence, AI)의 한계를 밀어냈다.</p>\n<ul>\n<li>\n<p>task 예시</p>\n<ul>\n<li>object recognition from images, speech recognition, statistical machine translation, Atari and Go games, abstract art, etc...</li>\n</ul>\n</li>\n</ul>\n<p><code class=\"language-text\">DNN</code>은 대부분 Graphic Processing Units (GPUs)을 사용하여 훈련되는데 <code class=\"language-text\">GPU</code>는 빠르지만, 전력을 많이 소비한다.<br>\n결과적으로 <code class=\"language-text\">DNN</code>을 <code class=\"language-text\">low-power device</code> 에서 구동하는 것은 어려운 일이 되었으며 <strong>대부분의 연구가 범용 및 특화된 컴퓨터 하드웨어 <code class=\"language-text\">run-time</code> 에서 <code class=\"language-text\">DNN</code>을 가속화 하는데 집중되고 있다.</strong></p>\n<h3 id=\"contributions\" style=\"position:relative;\"><a href=\"#contributions\" aria-label=\"contributions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Contributions</h3>\n<ol>\n<li>run-time에서 binary weights, activations 로 BNN (Binarized-Neural-Networks)를 훈련하는 방법과 train-time에서 gradients 를 계산 하는 방법을 제안</li>\n<li>\n<p>BNN으로 MNIST, CIFAR-10, SVHN을 훈련시켰으며, 거의 최상의 결과를 얻어냄</p>\n<ul>\n<li>BNN은 Torch7과 Theano 프레임워크를 사용하여 구성하였다.</li>\n</ul>\n</li>\n<li>\n<p>효율성의 증가</p>\n<ul>\n<li>3.1) forward pass (run-time, train-time) 동안 BNN 이 메모리 소비 (크기와 접근 횟수)를 급격하게 줄이는 것을 보여준다.</li>\n<li>3.2) 에너지 효율성과 시간 복잡도의 감소</li>\n<li>대부분의 산술 연산자를 비트 연산자로 줄여 에너지 효율성을 증가시킨다.</li>\n<li>binarized CNN 은 binalzed convolution kernel 의 반복으로 이어질 수 있다. 전용 하드웨어는 시간 복잡도를 60% 까지 감소시킬 수 있다고 예상된다.</li>\n</ul>\n</li>\n<li>\n<p>binary matrix multiplication GPU kernel 를 프로그래밍</p>\n<ul>\n<li>분류 정확도의 손실 없이, 최적화되지 않은 GPU 커널보다 MNIST BNN 을 7 배 빠르게 돌릴 수 있다.</li>\n</ul>\n</li>\n<li>BNN 을 위한 코드 (training and running)는 Torch와 Theano 프레임워크에서 on-line으로 실행이 가능</li>\n</ol>","excerpt":"Abstract BNNs (Binarized Neural Networks) neural networks with binary wrights and activations at run-time at trining-time the binary weights…","frontmatter":{"date":"2020-12-18","title":"Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1 (1)","tags":["AI","NN","review"],"keywords":null},"fields":{"slug":"/pages/bnn"},"tableOfContents":"<ul>\n<li><a href=\"/pages/bnn/#abstract\">Abstract</a></li>\n<li>\n<p><a href=\"/pages/bnn/#introduction\">Introduction</a></p>\n<ul>\n<li><a href=\"/pages/bnn/#contributions\">Contributions</a></li>\n</ul>\n</li>\n</ul>"}},"pageContext":{"slug":"/pages/bnn"}},"staticQueryHashes":["2827402515","694178885"]}